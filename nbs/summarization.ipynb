{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import yaml\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/NLP/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:1033: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/NLP/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7213a309ad4a0493a0b103d9195a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/NLP/lib/python3.9/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "config_path =  '../configs/model/Llama_2_7b.yaml'\n",
    "\n",
    "# Load configuration from the YAML file\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "model_id = config['model']['id']\n",
    "device = config['model']['device']\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=config['bits_and_bytes']['load_in_4bit'],\n",
    "    bnb_4bit_quant_type=config['bits_and_bytes']['bnb_4bit_quant_type'],\n",
    "    bnb_4bit_use_double_quant=config['bits_and_bytes']['bnb_4bit_use_double_quant'],\n",
    "    bnb_4bit_compute_dtype=getattr(torch, config['bits_and_bytes']['bnb_4bit_compute_dtype'])\n",
    ")\n",
    "\n",
    "hf_auth = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device if device != 'auto' else 'auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "actual_device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Model loaded on {actual_device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/NLP/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 29871, 13, 29950, 7889, 29901], [1, 29871, 13, 28956, 13]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    1, 29871,    13, 29950,  7889, 29901], device='cuda:0'),\n",
       " tensor([    1, 29871,    13, 28956,    13], device='cuda:0')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to('cuda') for x in stop_token_ids]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True, \n",
    "    task='text-generation',\n",
    "    stopping_criteria=stopping_criteria, \n",
    "    temperature=0.5, \n",
    "    max_new_tokens=512,\n",
    "    repetition_penalty=1.1 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "out = llm(prompt=\"Complete the following text: \\nThe quick brown fox ...\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A) jumped over a lazy dog.\n",
      "B) ran quickly to catch its prey.\n",
      "C) was chased by a pack of wolves.\n",
      "D) all of the above.\n",
      "\n",
      "Answer: D) all of the above.\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_from_disk(\"/home/ubuntu/oulas/github/text-summarization-pipeline/data/validation_split\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{document}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables = [\"document\"],template=prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sample = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(prompt.format(**{\"document\":validation_sample['document']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Write a concise summary of the following:\\n'\n",
      " '\"Draw a number line, but place your zero to the far right of the line. Then, '\n",
      " 'number your line toward your left using negative numbers. Mark the first '\n",
      " 'negative number you want to add on the number line. Then, count out your '\n",
      " 'second negative number, moving to the left. This gives you your answer. For '\n",
      " 'example, let’s say you’re adding -4+-6. You’d circle -4 on your number line. '\n",
      " 'Then, count 6 spaces to the left. You’ll arrive at -10, which is your '\n",
      " 'answer. This is because you are moving the same number of places on the '\n",
      " 'number line, just toward the negative side. This means your final answer '\n",
      " 'will be negative. For example, when adding -12+-21, you could add 12+21=33. '\n",
      " 'However, since your numbers were negative, you’d make your answer -33. Draw '\n",
      " 'a number line with zero in the middle. Number to the left moving from -1 to '\n",
      " '-10, then number to the right 1 to 10. Circle the positive number on your '\n",
      " 'number line. Then, count out your negative number to find the answer, moving '\n",
      " 'to the left on your number line. For instance, let’s say you’re adding 6+-8. '\n",
      " 'You’ll circle 6 on your number line. Then, count 8 spaces backwards, moving '\n",
      " 'to the left on your line. You’ll arrive at -2, which is your answer. Adding '\n",
      " 'a negative number with a positive number works just like subtraction. This '\n",
      " 'is because you’re taking away spaces on the number line. When you’re solving '\n",
      " 'a problem on paper, you can write it out like a subtraction problem. For '\n",
      " 'example, 15+-17 would become 15-17=-2.\"\\n'\n",
      " 'CONCISE SUMMARY:')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(prompt.format(**{\"document\":validation_sample['document']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = llm(prompt=prompt.format(**{\"document\":validation_sample['document']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " 'To perform addition with negative numbers, draw a number line with zero in '\n",
      " 'the middle and number it from -1 to 10 on both sides. Move the cursor or pen '\n",
      " 'to the left for negative numbers and right for positive numbers. When adding '\n",
      " 'a negative number with a positive number, move towards the left on the '\n",
      " 'number line and count the number of spaces. The answer will be negative.')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = 'sentence-transformers/sentence-t5-base'\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
