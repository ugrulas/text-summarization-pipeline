{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/NLP/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:1033: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b475de7f3a284b98884d0436ba015708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/NLP/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020cd086bfcf4e7e877500021f9fc870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c659d0fbce74dfb95eddbc1586d57b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bc2d9f81b34993a8eaa7b7febd5ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3960dfafa8504b72898f7fa7cf59b548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45a15943efe452494f59f49393f0bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/NLP/lib/python3.9/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7871517e864e15a99790eb5f369f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, you need an access token\n",
    "hf_auth = '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'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/NLP/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0843cdaf88cc49aa8909692ab76ac6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0789eed38a41009971b101e2127149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d5e49f8f5a4ace944372d4a629a606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00948fd353d44bbaa7d6fc595fdc8f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 29871, 13, 29950, 7889, 29901], [1, 29871, 13, 28956, 13]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    1, 29871,    13, 29950,  7889, 29901], device='cuda:0'),\n",
       " tensor([    1, 29871,    13, 28956,    13], device='cuda:0')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True, \n",
    "    task='text-generation',\n",
    "    stopping_criteria=stopping_criteria, \n",
    "    temperature=0.1, \n",
    "    max_new_tokens=512,\n",
    "    repetition_penalty=1.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = generate_text(\"The quick brown fox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog. Hinweis: This is a 50-letter word, so it's a good one to practice!\n",
      "\n",
      "Answer: The quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "out = llm(prompt=\"Complete the following text: \\nThe quick brown fox ...\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " nobody knows the exact origin of this pangram, but it has been used as a demonstration of a language's range and complexity since at least the 19th century. It is often cited as an example of a sentence that uses all the letters of the alphabet at least once.\n",
      "\n",
      "The word \"pangram\" comes from the Greek words \"pan\" meaning \"all\" and \"gramma\" meaning \"letter\". A pangram is a sentence or phrase that uses every letter of the alphabet at least once. Pangrams are often used in linguistics and language learning to demonstrate the range and complexity of a language.\n",
      "\n",
      "Here are some examples of pangrams in different languages:\n",
      "\n",
      "* English: The quick brown fox jumps over the lazy dog.\n",
      "* Spanish: El rápido perro gris saltó sobre el perro lento.\n",
      "* French: Le chien rapide brun saute sur le chien lent.\n",
      "* German: Der schnelle braune Hund springt über den langen Hund.\n",
      "* Chinese (Mandarin): 快速的灰色狗跳过懒犬。\n",
      "* Japanese: 速い灰色犬は、懶犬を跳び越します。\n",
      "\n",
      "Pangrams can be found in many languages, and they are often used as a tool for language learners to practice their writing skills.\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{document}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables = [\"document\"],template=prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(prompt.format(**{\"document\":validation_sample['document']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Write a concise summary of the following:\\n'\n",
      " '\"This is very important, as it will protect your hair from heat damage. '\n",
      " 'Heat-damaged hair can often look dry, frizzy, or ratted. To prevent this '\n",
      " 'from happening, apply a heat protectant spray to your hair, focusing on the '\n",
      " 'ends. You should have the bottom third or bottom fourth of your hair loose. '\n",
      " 'The thicker you hair is, the more sections you will need to work with. Take '\n",
      " 'a small section of hair, and place a round brush under it, as close to your '\n",
      " 'roots as you can. Turn on your hairdryer, and place it right over the brush. '\n",
      " 'Keep the hairdryer directly over the brush at all times. Do not pull the '\n",
      " 'brush completely past the ends of your hair. If your hair is not smooth, '\n",
      " 'pull the brush completely past your hair and start again. When your hair is '\n",
      " 'smooth, rotate the brush back towards your scalp, rolling the hair around '\n",
      " 'it. Leave the brush in your hair as it cools. Once it is cool to the touch, '\n",
      " 'carefully pull the brush out of your hair. At this point, you can move onto '\n",
      " 'the next section.  If your hair is very fine, consider giving it a quick '\n",
      " 'spray with a lightweight hairspray before moving on. Save power by turning '\n",
      " 'your hairdryer off while you wait for your hair to cool. Keep working in '\n",
      " 'small sections and thin layers until you have straightened all of your hair. '\n",
      " 'If you are happy with this look, you are done and can go about your day. If '\n",
      " \"your hair isn't straight enough for you, pass a hair straightener through \"\n",
      " 'the ends and the upper layers. Pull the hair straightener through your hair '\n",
      " 'using an upwards motion rather than straight down. This will help create '\n",
      " 'extra volume. Here are some more pointers on straightening your hair:  For '\n",
      " 'even more volume: roll your hair up in a round brush or thermal styling '\n",
      " 'brush, about 1½ to 2 inches (3.81 to 5.08 centimeters) away from your scalp. '\n",
      " 'Keep the brush in your hair until your hair cools before pulling it out. '\n",
      " 'Pass your hair straightener through your hair, right under the brush, while '\n",
      " 'unrolling it to make your hair even straighter. Run a brush or your fingers '\n",
      " 'through your hair, starting at your forehead, and moving straight back. This '\n",
      " 'will help create extra volume. After that, you can part or style your hair '\n",
      " 'however you like.\"\\n'\n",
      " 'CONCISE SUMMARY:')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(prompt.format(**{\"document\":validation_sample['document']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = llm(prompt=prompt.format(**{\"document\":validation_sample['document']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " 'To prevent heat damage when straightening your hair, apply a heat protectant '\n",
      " 'spray to the ends of your hair and focus on the lower sections. Use a round '\n",
      " 'brush and a hairdryer to straighten your hair in small sections, keeping the '\n",
      " 'brush close to your roots. Repeat the process until all of your hair is '\n",
      " 'straightened, then use a hair straightener to add extra volume.')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
